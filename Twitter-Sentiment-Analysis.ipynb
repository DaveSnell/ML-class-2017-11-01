{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell lines are just to read your personal Twitter parameters from a file\n",
    "instead of hardcoding them into your program. A still better approach would be to encrypt them\n",
    "in the file and decrypt them in the program. Note that readline automatically appends a newline charater, so we strip it off with the .strip('\\n') function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meFilename = 'me.txt'\n",
    "meFile = open(meFilename,'r') # add a pathname if needed\n",
    "consumer_key = str(meFile.readline()).strip('\\n')\n",
    "consumer_secret = meFile.readline().strip('\\n')\n",
    "access_token = meFile.readline().strip('\\n')\n",
    "access_token_secret = meFile.readline().strip('\\n')\n",
    "meFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be something like the following (but with the values from your text file):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consumer_key = 'UIiKZVqFAlBZAyxdIuMK7CMtDz'\n",
    "\n",
    "consumer_secret = 'loiPCSBlZVvgzrFFJ5ZaA0DOzMeaQ8dvJ4Qqg5i80wUSCHXRKKb'\n",
    "\n",
    "access_token = '550502130-CAu3LZrUoDcLNG4kAUu4swgKAAGn7Ap9FBSelaAn6'\n",
    "\n",
    "access_token_secret = 'EP6nX8nP4FSoEj2gu47mGPrB43nVRxn6J8vFWeAUBsHS8S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get into the Halloween spirit with these geeked out pumpkins üéÉ https://t.co/63DZvcD9gR https://t.co/ghM4Agp2FP\n",
      "I wish the self-defeating stupidity of the UK voting Brexit was as easily reversible as the self-defeating stupidity of the US voting Trump.\n",
      "Want to transport yourself to another world? These sci-fi authors have got you covered. https://t.co/p5BLgsVu1R\n",
      "Jeepers creepers! It's a creature double feature! https://t.co/I4jhRVvaRu https://t.co/mXrrd74WsZ\n",
      "RT @InternetHippo: Terrorist: The code to disarming the nuke is at the end of this 280-character tweet. You need only read it\n",
      "\n",
      "Me: Folks we‚Ä¶\n",
      "lol hugely flattered, the WikiTribune team have been given my t-shirts https://t.co/i5d6t1E03S\n",
      "\n",
      "Get yours here!‚Ä¶ https://t.co/qCD57wt8Bm\n",
      "RT @bechter: Apparently Warheads get their sourness from malic acid, which can also be found in Granny Smith apples. Huh! https://t.co/L3yG‚Ä¶\n",
      "RT @annieminoff: Just another day at the @scifri office, eating vomit candy w/ flavor historian @thebirdisgone. Glad we had spit cups https‚Ä¶\n",
      "Really sad for patients with psychosis to see the BPS apparently engaged in a clumsy sectarian ideological misfire.  https://t.co/6NLEyuAlEM\n",
      "I don't believe you lol https://t.co/a9ahtYLCni\n"
     ]
    }
   ],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "maxTweets = 10\n",
    "tweetNum = 0\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    if tweetNum < maxTweets:\n",
    "        tweetNum +=1 \n",
    "        print (tweet.text)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the User object for twitter...\n",
    "user = api.get_user('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6566\n",
      "User(_api=<tweepy.api.API object at 0x7fc9b71ccb38>, _json={'id': 783214, 'id_str': '783214', 'name': 'Twitter', 'screen_name': 'Twitter', 'location': 'San Francisco, CA', 'profile_location': None, 'description': 'Your official source for what‚Äôs happening.\\n\\nNeed a hand? Visit https://t.co/jTMg7YsLw5', 'url': 'https://t.co/gN5JJwhQy7', 'entities': {'url': {'urls': [{'url': 'https://t.co/gN5JJwhQy7', 'expanded_url': 'https://blog.twitter.com/', 'display_url': 'blog.twitter.com', 'indices': [0, 23]}]}, 'description': {'urls': [{'url': 'https://t.co/jTMg7YsLw5', 'expanded_url': 'https://support.twitter.com', 'display_url': 'support.twitter.com', 'indices': [63, 86]}]}}, 'protected': False, 'followers_count': 62056962, 'friends_count': 170, 'listed_count': 90700, 'created_at': 'Tue Feb 20 14:35:54 +0000 2007', 'favourites_count': 5197, 'utc_offset': -25200, 'time_zone': 'Pacific Time (US & ...\n",
      "\n",
      "...nd_color='ACDED6', profile_background_image_url='http://pbs.twimg.com/profile_background_images/657090062/l1uqey5sy82r9ijhke1i.png', profile_background_image_url_https='https://pbs.twimg.com/profile_background_images/657090062/l1uqey5sy82r9ijhke1i.png', profile_background_tile=True, profile_image_url='http://pbs.twimg.com/profile_images/875087697177567232/Qfy0kRIP_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/875087697177567232/Qfy0kRIP_normal.jpg', profile_banner_url='https://pbs.twimg.com/profile_banners/783214/1503522865', profile_link_color='1B95E0', profile_sidebar_border_color='FFFFFF', profile_sidebar_fill_color='F6F6F6', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=True, default_profile=False, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='regular')\n"
     ]
    }
   ],
   "source": [
    "u = str(user)\n",
    "print(len(u))\n",
    "print(u[:900] + '...')\n",
    "print()\n",
    "print('...' + u[-900:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter\n",
      "62056962\n"
     ]
    }
   ],
   "source": [
    "print (user.screen_name)\n",
    "print (user.followers_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis using tweepy,NLTK and Textblob\n",
    "\n",
    "Using OAuth to make connection twitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import os\n",
    "import re\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting tweets in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('python.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key attributes of the tweets pulled out are :\n",
    "\n",
    "    text: the text of the tweet itself\n",
    "\n",
    "    created_at: the date of creation\n",
    "\n",
    "    favorite_count, retweet_count: the number of favourites and retweets\n",
    "\n",
    "favorited, retweeted: boolean stating whether the authenticated user (you) have favourited or retweeted this tweet\n",
    "\n",
    "lang: acronym for the language (e.g. ‚Äúen‚Äù for english)\n",
    "\n",
    "id: the tweet identifier\n",
    "\n",
    "place, coordinates, geo: geo-location information if available\n",
    "\n",
    "user: the author‚Äôs full profile\n",
    "\n",
    "entities: list of entities like URLs, @-mentions, hashtags and symbols\n",
    "\n",
    "in_reply_to_user_id: user identifier if the tweet is a reply to a specific user\n",
    "\n",
    "in_reply_to_status_id: status identifier id the tweet is a reply to a specific status\n",
    "\n",
    "Processing tweets\n",
    "\n",
    "Tokenizing the tweet\n",
    "\n",
    "Tokenizing @-mentions, emoticons, URLs and #hash-tags as individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expressions are compiled with the flags re.VERBOSE, to allow spaces in the regexp to be ignored (see the multi-line emoticons regexp), and re.IGNORECASE to catch both upper and lowercases. Thetokenize() function simply catches all the tokens in a string and returns them as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop-words,punctuations and rt,via words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep track of the frequencies while we are processing the tweets, we can use collections.Counter() which internally is a dictionary (term: count) with some useful methods like most_common():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'python.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ad7b2d29ad08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcount_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Create a list with all the terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'python.json'"
     ]
    }
   ],
   "source": [
    "with open('python.json', 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        # Update the counter\n",
    "\t    #terms_single = set(terms_all)\n",
    "\t\t# Count hashtags only\n",
    "        terms_hash = [term for term in preprocess(tweet['text']) \n",
    "              if term.startswith('#')]\n",
    "# Count terms only (no hashtags, no mentions)\n",
    "        terms_only = [term for term in preprocess(tweet['text']) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))] \n",
    "              # mind the ((double brackets))\n",
    "              # startswith() takes a tuple (not a list) if \n",
    "              # we pass a list of inputs\n",
    "####Python TextBlob Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis refers to the process of taking natural language to identify and extract subjective information. You can take text, run it through the TextBlob and the program will spit out if the text is positive, neutral, or negative by analyzing the language used in the text.\n",
    "\n",
    "Sentiment Analysis\t\n",
    "Text\tIf that is not cool enough for you than that is a you problem.\n",
    "Polarity\t-0.0875\n",
    "Subjectivity\t0.575\n",
    "Classification\tneg\n",
    "P_Pos\t0.344455873\n",
    "P_Neg\t0.655544127\n",
    "What does that mean?\n",
    "\n",
    "Polarity - a measure of the negativity, the neutralness, or the positivity of the text\n",
    "Classification - either pos or neg indicating if the text is positive or negative\n",
    "To calculate the overall sentiment, we look at the polarity score:\n",
    "    Positive ‚Äì from .01 to 1\n",
    "    Neutral ‚Äì 0\n",
    "    Negative ‚Äì from ‚Äì.01 to -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        blob = TextBlob(tweet[\"text\"])\n",
    "        cout+=1\n",
    "        lis.append(blob.sentiment.polarity)\n",
    "        #print blob.sentiment.subjectivity\n",
    "        #print (os.listdir(tweet[\"text\"]))\n",
    "        if blob.sentiment.polarity < 0:\n",
    "            sentiment = \"negative\"\n",
    "            neg+=blob.sentiment.polarity\n",
    "            n+=1\n",
    "        elif blob.sentiment.polarity == 0:\n",
    "            sentiment = \"neutral\"\n",
    "            net+=1\n",
    "        else:\n",
    "            sentiment = \"positive\"\n",
    "            pos+=blob.sentiment.polarity\n",
    "            p+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
